{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "# Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pyJHTDB\n",
    "import h5py\n",
    "import time as tt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import filters\n",
    "import skimage.morphology as morphology\n",
    "from skimage.morphology import square\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import cm\n",
    "import scipy.io as sio\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.animation as animation\n",
    "# load shared library\n",
    "lTDB = pyJHTDB.libJHTDB()\n",
    "#initialize webservices\n",
    "lTDB.initialize()\n",
    "\n",
    "#Add token\n",
    "auth_token  = \"edu.jhu.burgers-2e898d7f\"\n",
    "lTDB.add_token(auth_token)\n",
    "\n",
    "dataset = 'transition_bl'\n",
    "\n",
    "spatialInterp  = 0  # no spatial interpolation\n",
    "temporalInterp = 0  # no time interpolation\n",
    "FD4NoInt       = 40 # 4th-order FD, no spatial interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database domain size and number of grid points\n",
    "x_min =   30.2185\n",
    "x_max = 1000.0650\n",
    "y_min =    0.0036\n",
    "y_max =   26.4880\n",
    "z_min =    0.0000\n",
    "z_max =  240.0000\n",
    "d99i  =    0.9648\n",
    "d99f  =   15.0433\n",
    "\n",
    "nx = 3320\n",
    "ny =  224\n",
    "nz = 2048\n",
    "\n",
    "# Database time duration\n",
    "Ti = 0\n",
    "Tf = Ti + 1175\n",
    "dt = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define domain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create surface\n",
    "# nix = round(nx / 10)\n",
    "# niz = round(nz / 10)\n",
    "# niy = round(ny / 1)\n",
    "# x = np.linspace(x_min, x_max, nix)\n",
    "# z = np.linspace(z_min, z_max, niz)\n",
    "# y = np.linspace(y_min, y_max, niy); #d99i\n",
    "\n",
    "# [X,Z] = np.meshgrid(x,z)\n",
    "# print(niz,nix,niy,nix*niy*niz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getVelocity  &  getVelocityGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get the velocity at each point\n",
    "\n",
    "# time = 1000\n",
    "# ui=np.zeros((nix,niz,niy,3))\n",
    "# duidxj=np.zeros((nix,niz,niy,9))\n",
    "\n",
    "# for j in range(200,224):\n",
    "#     print('Read y:',j,y[j])\n",
    "#     start = tt.time()\n",
    "#     points  = np.zeros((nix,niz,3))\n",
    "#     points[:,:,0] = X.transpose()\n",
    "#     points[:,:,1] = y[j]\n",
    "#     points[:,:,2] = Z.transpose()\n",
    "\n",
    "#     # 3D array with single precision values\n",
    "#     points = np.array(points,dtype='float32')\n",
    "\n",
    "#     print('Requesting velocity at {0} points...'.format(nix*niz*1))\n",
    "#     result = lTDB.getData(time, points, data_set = 'transition_bl',\n",
    "#             sinterp = spatialInterp, tinterp = temporalInterp,\n",
    "#             getFunction = 'getVelocity')\n",
    "#     end = tt.time()\n",
    "#     print('   '+str(end - start)+' seconds')\n",
    "\n",
    "#     start = tt.time()\n",
    "#     print('Requesting velocity gradients at {0} points...'.format(nix*niz*1))\n",
    "#     result_grad = lTDB.getData(time, points, data_set = 'transition_bl',\n",
    "#             sinterp = FD4NoInt, tinterp = temporalInterp,\n",
    "#             getFunction = 'getVelocityGradient')\n",
    "#     end = tt.time()\n",
    "#     print('   '+str(end - start)+' seconds')\n",
    "#     ui[0:nix,0:niz,j,0:3]=result[0:nix,0:niz,0:3]\n",
    "#     duidxj[0:nix,0:niz,j,0:9]=result_grad[0:nix,0:niz,0:9]\n",
    "    \n",
    "    \n",
    "    \n",
    "# #Save in matlab format   \n",
    "# #sio.savemat('data_t1000_415x28x256.mat',{'x':x,'y':y,'z':z,'u_i':result,'duidxj':result_grad})\n",
    "\n",
    "# #save using hdf5 format\n",
    "# h5f = h5py.File('data_t1000_332x224x205_224.h5', 'w')\n",
    "# h5f.create_dataset('x', data=X)\n",
    "# h5f.create_dataset('y', data=y)\n",
    "# h5f.create_dataset('z', data=Z)\n",
    "# h5f.create_dataset('ui', data=ui)\n",
    "# h5f.create_dataset('duidxj', data=duidxj)\n",
    "# h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity and velocity gradient input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sio.loadmat('data_t0_224x205x332.mat')\n",
    "u=data['u'][0::5,0::7,0::7]\n",
    "v=data['v'][0::5,0::7,0::7]\n",
    "w=data['w'][0::5,0::7,0::7]\n",
    "up=data['up'][0::5,0::7,0::7]\n",
    "vp=data['vp'][0::5,0::7,0::7]\n",
    "dudx=data['dudx'][0::5,0::7,0::7]\n",
    "dudy=data['dudy'][0::5,0::7,0::7]\n",
    "dudz=data['dudz'][0::5,0::7,0::7]\n",
    "\n",
    "dvdx=data['dvdx'][0::5,0::7,0::7]\n",
    "dvdy=data['dvdy'][0::5,0::7,0::7]\n",
    "dvdz=data['dvdz'][0::5,0::7,0::7]\n",
    "\n",
    "dwdx=data['dwdx'][0::5,0::7,0::7]\n",
    "dwdy=data['dwdy'][0::5,0::7,0::7]\n",
    "dwdz=data['dwdz'][0::5,0::7,0::7]\n",
    "\n",
    "x=data['x'][0::7]\n",
    "y=data['y'][0::5]\n",
    "z=data['z'][0::7]\n",
    "[X,Y,Z]=np.meshgrid(x,y,z)\n",
    "nix=x.shape[0]\n",
    "niy=y.shape[0]\n",
    "niz=z.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45, 30, 48), 45, 30, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape,niy,niz,nix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input vector definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.zeros((nix*niz*niy, 16), np.float32)      # Size of input vector (u,v,w,up,vp, du/dxyz,dv/dxyz, dw/dxyz, x,y)\n",
    "input_vector_zhao = np.zeros((nix*niz*niy, 16), np.float32)      # Size of input vector (u,v,w,up,vp, du/dxyz,dv/dxyz, dw/dxyz, x,y)\n",
    "std_vector = np.zeros((1, 16), np.float32)      # Size of input vector (u,v,w,up,vp, du/dxyz,dv/dxyz, dw/dxyz, x,y)\n",
    "input_vector[:,0]=np.abs(u[:,:,:]).flatten()               # u\n",
    "input_vector[:,1]=np.abs(v[:,:,:]).flatten()               # v\n",
    "input_vector[:,2]=np.abs(w[:,:,:]).flatten()               # w\n",
    "input_vector[:,3]=np.abs(up[:,:,:]).flatten()              # up\n",
    "input_vector[:,4]=np.abs(vp[:,:,:]).flatten()              # vp\n",
    "input_vector[:,5]=np.abs(dudx[:,:,:]).flatten()            # dudx\n",
    "input_vector[:,6]=np.abs(dudy[:,:,:]).flatten()            # dudy\n",
    "input_vector[:,7]=np.abs(dudz[:,:,:]).flatten()            # dudz\n",
    "input_vector[:,8]=np.abs(dvdx[:,:,:]).flatten()            # dvdx\n",
    "input_vector[:,9]=np.abs(dvdy[:,:,:]).flatten()            # dvdy\n",
    "input_vector[:,10]=np.abs(dvdz[:,:,:]).flatten()           # dvdz\n",
    "input_vector[:,11]=np.abs(dwdx[:,:,:]).flatten()           # dwdx\n",
    "input_vector[:,12]=np.abs(dwdy[:,:,:]).flatten()           # dwdy\n",
    "input_vector[:,13]=np.abs(dwdz[:,:,:]).flatten()           # dwdz\n",
    "input_vector[:,14]=np.abs(X[:,:,:]).flatten()              # x\n",
    "input_vector[:,15]=np.abs(Y[:,:,:]).flatten()              # y\n",
    "\n",
    "#Standard deviation and coefficients from Zhao et al 2019\n",
    "std_vector_zhao=np.array([[0.1312,0.0229,0.0264,0.04,0.0229,0.0243,0.1595,0.0626,\n",
    "               0.0246,0.0296,0.0385,0.0270,0.0536,0.0306,286.6778,8.2739]])\n",
    "coeffs_zhao=np.array([0.19, -0.15, -0.16, -0.16, -0.16, -0.17, -0.10, -0.15,\n",
    "                -0.15, -0.17, -0.16, -0.17, -0.16, -0.17, -0.08, 0.15])\n",
    "for i in range(input_vector.shape[1]):\n",
    "    std_vector[0,i]=np.std(input_vector[:,i])\n",
    "    input_vector_zhao[:,i]=input_vector[:,i]/std_vector_zhao[0,i]  # Normalize by standard devition\n",
    "    input_vector[:,i]=input_vector[:,i]/std_vector[0,i]  # Normalize by standard devition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance 0.9999999 1.8752782\n",
      "variance 1.0 0.59614605\n",
      "variance 0.9999999 0.6413575\n",
      "variance 1.0 0.65841883\n",
      "variance 0.9999999 0.58708555\n",
      "variance 1.0000001 0.931605\n",
      "variance 1.0000002 2.7819993\n",
      "variance 1.0000001 0.914742\n",
      "variance 1.0000001 0.94644463\n",
      "variance 0.99999994 0.8216153\n",
      "variance 1.0000001 0.9596662\n",
      "variance 1.0000001 0.9380482\n",
      "variance 0.99999994 1.7595795\n",
      "variance 0.9999998 0.85986406\n",
      "variance 1.0 0.98235995\n",
      "variance 0.9999999 0.86879903\n"
     ]
    }
   ],
   "source": [
    "#Check if varaiance of input data is 1\n",
    "for i in range(input_vector.shape[1]):\n",
    "    print('variance',np.var(input_vector[:,i]),np.var(input_vector_zhao[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data using Kmeans clustering (SOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning training...\n",
      "   0.5161142349243164 seconds\n"
     ]
    }
   ],
   "source": [
    "#Encode data as T or NT using KMeans clustering (Zhao et al, 2018) \n",
    "start = tt.time()\n",
    "print('Machine learning training...')\n",
    "kmeans = KMeans(n_clusters=2).fit(input_vector)\n",
    "end = tt.time()\n",
    "print('   '+str(end - start)+' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07276528 -0.10107189 -0.10450955 -0.10480831 -0.10204438 -0.10837419\n",
      " -0.02022405 -0.09381035 -0.10640237 -0.11023948 -0.10315286 -0.11028725\n",
      " -0.0743276  -0.11148015 -0.00426443  0.08950542]\n"
     ]
    }
   ],
   "source": [
    "#Finding co-efficients\n",
    "#mask=kmeans.labels_.reshape((niz,nix,niy))\n",
    "\n",
    "a=kmeans.cluster_centers_[0]\n",
    "b=kmeans.cluster_centers_[1]\n",
    "c=np.zeros(a.shape[0])\n",
    "coeffs=np.zeros(a.shape[0])\n",
    "\n",
    "for i in range(a.shape[0]):\n",
    "    c[i]=0.5*(a[i]+b[i])\n",
    "B=0\n",
    "for i in range(c.shape[0]):\n",
    "    B=B-c[i]*(b[i]-a[i])\n",
    "    \n",
    "#co-efficients\n",
    "for i in range(c.shape[0]):\n",
    "    coeffs[i]=(b[i]-a[i])/B\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare weights with Zhao et al, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07276528 -0.10107189 -0.10450955 -0.10480831 -0.10204438 -0.10837419\n",
      " -0.02022405 -0.09381035 -0.10640237 -0.11023948 -0.10315286 -0.11028725\n",
      " -0.0743276  -0.11148015 -0.00426443  0.08950542]\n",
      "[ 0.19 -0.15 -0.16 -0.16 -0.16 -0.17 -0.1  -0.15 -0.15 -0.17 -0.16 -0.17\n",
      " -0.16 -0.17 -0.08  0.15]\n"
     ]
    }
   ],
   "source": [
    "print(coeffs)\n",
    "print(coeffs_zhao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the T-NT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-NT classification of training data\n",
    "mask=np.zeros((nix*niz*niy),np.int32)\n",
    "y_true=[]\n",
    "for i in range(nix*niz*niy):\n",
    "    p=1\n",
    "    for k in range(16):\n",
    "        p=p+coeffs[k]*input_vector[i,k]\n",
    "    if p<0:\n",
    "        mask[i]=1#turbulent\n",
    "        y_true.append(\"T\")\n",
    "    elif p>0:\n",
    "        mask[i]=0#laminar\n",
    "        y_true.append(\"NT\")\n",
    "        \n",
    "mask = mask.reshape((niy,niz,nix))\n",
    "#sio.savemat('mask_t0_SOM_45x30x48.mat',{'mask_t0_SOM':mask,'coeffs_t0_SOM':coeffs,'y_true_t0':y_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data in HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5f = h5py.File('data_t0_TNT_45x30x48.h5', 'w')\n",
    "# h5f.create_dataset('u',data=input_vector[:,0])           \n",
    "# h5f.create_dataset('v',data=input_vector[:,1])           \n",
    "# h5f.create_dataset('w',data=input_vector[:,2])\n",
    "# h5f.create_dataset('up',data=input_vector[:,3])\n",
    "# h5f.create_dataset('vp',data=input_vector[:,4])\n",
    "# h5f.create_dataset('dudx',data=input_vector[:,5])\n",
    "# h5f.create_dataset('dudy',data=input_vector[:,6])\n",
    "# h5f.create_dataset('dudz',data=input_vector[:,7])\n",
    "# h5f.create_dataset('dvdx',data=input_vector[:,8])\n",
    "# h5f.create_dataset('dvdy',data=input_vector[:,9])\n",
    "# h5f.create_dataset('dvdz',data=input_vector[:,10])\n",
    "# h5f.create_dataset('dwdx',data=input_vector[:,11])\n",
    "# h5f.create_dataset('dwdy',data=input_vector[:,12])\n",
    "# h5f.create_dataset('dwdz',data=input_vector[:,13])\n",
    "# h5f.create_dataset('x',data=input_vector[:,14])\n",
    "# h5f.create_dataset('y',data=input_vector[:,15])\n",
    "# h5f.create_dataset('Z',data=Z)\n",
    "# h5f.create_dataset('Y',data=Y)\n",
    "# h5f.create_dataset('X',data=X)\n",
    "# y_data = np.array(y_true, dtype=object)\n",
    "# string_dt = h5py.special_dtype(vlen=str)\n",
    "# h5f.create_dataset('TNT',data=y_data,dtype=string_dt)\n",
    "# h5f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
